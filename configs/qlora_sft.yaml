model_name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
load_in_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"

lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
max_seq_length: 2048
bf16: true
gradient_checkpointing: true
optim: "adamw_8bit"

dataset_path: "data/sft_dataset"
image_column: "image"

output_dir: "outputs/sft-v1"
save_strategy: "steps"
save_steps: 100
logging_steps: 10
